{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pyreadr as py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam, RMSprop\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Data downloading\n",
    "Data link  \n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/6C3JR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data description\n",
    "Here we consoder dataset of \"Additional Tennessee Eastman Process Simulation Data for Anomaly Detection Evaluation\"\n",
    "This dataverse contains the data referenced in Rieth et al. (2017). Issues and Advances in Anomaly Detection Evaluation for Joint Human-Automated Systems. To be presented at Applied Human Factors and Ergonomics 2017.\n",
    "##### Columns description\n",
    "* **faultNumber** ranges from 1 to 20 in the “Faulty” datasets and represents the fault type in the TEP. The “FaultFree” datasets only contain fault 0 (i.e. normal operating conditions).\n",
    "* **simulationRun** ranges from 1 to 500 and represents a different random number generator state from which a full TEP dataset was generated (Note: the actual seeds used to generate training and testing datasets were non-overlapping).\n",
    "* **sample** ranges either from 1 to 500 (“Training” datasets) or 1 to 960 (“Testing” datasets). The TEP variables (columns 4 to 55) were sampled every 3 minutes for a total duration of 25 hours and 48 hours respectively. Note that the faults were introduced 1 and 8 hours into the Faulty Training and Faulty Testing datasets, respectively.\n",
    "* **columns 4-55** contain the process variables; the column names retain the original variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! unzip ../../data/raw/dataverse_files.zip -d ../../data/raw/dataverse_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading train data in .R format\n",
    "a1 = py.read_r(\"../../data/raw/dataverse_files/TEP_FaultFree_Training.RData\")\n",
    "a2 = py.read_r(\"../../data/raw/dataverse_files/TEP_Faulty_Training.RData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects that are present in a1 : odict_keys(['fault_free_training'])\n",
      "Objects that are present in a2 : odict_keys(['faulty_training'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Objects that are present in a1 :\", a1.keys())\n",
    "print(\"Objects that are present in a2 :\", a2.keys())\n",
    "# print(\"Objects that are present in a3 :\", a3.keys())\n",
    "# print(\"Objects that are present in a4 :\", a4.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinating the train and the test dataset\n",
    "\n",
    "raw_train = pd.concat([a1['fault_free_training'], a2['faulty_training']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5250000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5.250.000, 10.080.000\n",
    "len(raw_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Train-test-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "        'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8',\n",
    "        'xmeas_9', 'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', \n",
    "        'xmeas_17', 'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', \n",
    "        'xmeas_25', 'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32',\n",
    "        'xmeas_33', 'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', \n",
    "        'xmeas_41', 'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', \n",
    "        'xmv_10', 'xmv_11'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train['index'] = raw_train['faultNumber'] * 500 + raw_train['simulationRun'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_idx = raw_train[['index', 'faultNumber']].drop_duplicates()\n",
    "\n",
    "X_train_idx, X_val_idx = train_test_split(simulation_idx['index'], \n",
    "                                          stratify=simulation_idx['faultNumber'],\n",
    "                                          test_size=0.2, \n",
    "                                          random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = raw_train[raw_train['index'].isin(X_train_idx)].drop('index', axis=1)\n",
    "X_val = raw_train[raw_train['index'].isin(X_val_idx)].drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[features])\n",
    "\n",
    "X_train[features] = scaler.transform(X_train[features])\n",
    "X_val[features] = scaler.transform(X_val[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(y_pred, target):\n",
    "    \n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = torch.max(y_pred, dim=1)[1]  \n",
    "    \n",
    "    return torch.eq(y_pred, target).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAULT_START_TRAINVAL = 20\n",
    "FAULT_START_TEST = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTEP(Dataset):\n",
    "\n",
    "    def __init__(self, X):\n",
    "    \n",
    "        self.X = X\n",
    "        self.X = self.X.sort_values(['faultNumber', 'simulationRun', 'sample'])\n",
    "        self.X['index'] = self.X.groupby(['faultNumber', 'simulationRun']).ngroup()\n",
    "        self.X = self.X.set_index('index')\n",
    "        \n",
    "        self.max_length = self.X['sample'].max()\n",
    "\n",
    "        self.s_list = [FAULT_START_TRAINVAL, FAULT_START_TRAINVAL + 200]\n",
    "        self.l_list = [5, 25, 50, 100, 250]\n",
    "        \n",
    "#         self.s_list = [FAULT_START_TRAINVAL]\n",
    "#         self.l_list = [50, 100]\n",
    "        \n",
    "        self.features = [\n",
    "                'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', \n",
    "                'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', \n",
    "                'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', \n",
    "                'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', \n",
    "                'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', \n",
    "                'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11'\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.index.nunique() * len(self.s_list) * len(self.l_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        fault_sim_idx = idx // (len(self.s_list) * len(self.l_list))\n",
    "    \n",
    "        start_length_idxs = idx % (len(self.s_list) * len(self.l_list))\n",
    "        \n",
    "        start_idx = self.s_list[start_length_idxs // len(self.l_list)]\n",
    "        seq_length = self.l_list[start_length_idxs % len(self.l_list)]\n",
    "\n",
    "        features = self.X.loc[fault_sim_idx][self.features].values[start_idx : (start_idx+seq_length), :]\n",
    "        target = self.X.loc[fault_sim_idx]['faultNumber'].unique()[0]\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float)\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    sequences = [x[0] for x in batch]\n",
    "    labels = [x[1] for x in batch]\n",
    "        \n",
    "    lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "    lengths, idx = lengths.sort(0, descending=True)\n",
    "    \n",
    "    sequences = [sequences[i] for i in idx]\n",
    "    \n",
    "    labels = torch.tensor(labels, dtype=torch.long)[idx]\n",
    "    \n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "\n",
    "    return sequences_padded, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = DataTEP(X_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "val_ds = DataTEP(X_val)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE*4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84000, 21000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniRNN(nn.Module) :\n",
    "    def __init__(self, RNN_TYPE, NUM_LAYERS, INPUT_SIZE, HIDDEN_SIZE, LINEAR_SIZE, OUTPUT_SIZE, BIDIRECTIONAL, DESCRIPTION):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_type = RNN_TYPE\n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.linear_size = LINEAR_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.bidirectional = BIDIRECTIONAL\n",
    "        self.description = DESCRIPTION\n",
    "        \n",
    "        rnn_cell = getattr(nn, RNN_TYPE)\n",
    "        \n",
    "        self.rnn = rnn_cell(\n",
    "                        input_size=self.input_size, \n",
    "                        hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        batch_first=True,\n",
    "                        dropout=0.4\n",
    "                )    \n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Linear(in_features=self.hidden_size*(self.bidirectional+1), out_features=self.linear_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(in_features=self.linear_size, out_features=self.output_size),\n",
    "                )\n",
    "    \n",
    "    def get_params(self):\n",
    "        \n",
    "        return {\n",
    "            \"RNN_TYPE\": self.rnn_type,\n",
    "            \"HIDDEN_SIZE\": self.hidden_size,\n",
    "            \"NUM_LAYERS\": self.num_layers,\n",
    "            \"INPUT_SIZE\": self.input_size,\n",
    "            \"LINEAR_SIZE\": self.linear_size,\n",
    "            \"OUTPUT_SIZE\": self.output_size,\n",
    "            \"BIDIRECTIONAL\": self.bidirectional,\n",
    "            \"DESCRIPTION\": self.description,\n",
    "            }\n",
    "            \n",
    "    def forward(self, x, x_length):\n",
    "        \n",
    "        x_packed = pack_padded_sequence(x, x_length, batch_first=True)\n",
    "        x_rnn_out, _ = self.rnn(x_packed)\n",
    "        x_unpacked, _ = pad_packed_sequence(x_rnn_out, batch_first=True)\n",
    "        \n",
    "        idx_last_hidden = (x_length - 1).view(-1, 1).expand(len(x_length), x_unpacked.size(2)).unsqueeze(1)\n",
    "        idx_last_hidden = idx_last_hidden.to(x.device)\n",
    "        x_last_hiddens = x_unpacked.gather(1, idx_last_hidden).squeeze(1)\n",
    "        \n",
    "        x = self.head(x_last_hiddens)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(self, RNN_TYPE, NUM_LAYERS, INPUT_SIZE, HIDDEN_SIZE, LINEAR_SIZE, OUTPUT_SIZE, BIDIRECTIONAL):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.rnn_type = RNN_TYPE\n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.linear_size = LINEAR_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.bidirectional = BIDIRECTIONAL\n",
    "        \n",
    "        rnn_cell = getattr(nn, RNN_TYPE)\n",
    "\n",
    "        self.rnn = rnn_cell(\n",
    "                        input_size=self.input_size, \n",
    "                        hidden_size=self.hidden_size, \n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        dropout=0.4,\n",
    "                        batch_first=True\n",
    "                )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "#                         nn.Linear(in_features=self.hidden_size*(self.bidirectional+1), out_features=self.output_size),\n",
    "                        nn.Linear(in_features=self.hidden_size*(self.bidirectional+1), out_features=self.linear_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(in_features=self.linear_size, out_features=self.output_size),\n",
    "                )\n",
    "        \n",
    "        \n",
    "    def get_params(self):\n",
    "        \n",
    "        return {\n",
    "            \"RNN_TYPE\": self.rnn_type, \n",
    "            \"HIDDEN_SIZE\": self.hidden_size,\n",
    "            \"NUM_LAYERS\": self.num_layers,\n",
    "            \"INPUT_SIZE\": self.input_size,\n",
    "            \"LINEAR_SIZE\": self.linear_size,\n",
    "            \"OUTPUT_SIZE\": self.output_size,\n",
    "            \"BIDIRECTIONAL\": self.bidirectional\n",
    "        }\n",
    "    \n",
    "\n",
    "    def attention(self, lstm_output, last_hidden):\n",
    "        \n",
    "        attn_weights = torch.bmm(lstm_output, last_hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def forward(self, x, x_length):\n",
    "\n",
    "        x_packed = pack_padded_sequence(x, x_length, batch_first=True)\n",
    "        \n",
    "        x_rnn_out, _ = self.rnn(x_packed)\n",
    "        \n",
    "        x_unpacked, __ = pad_packed_sequence(x_rnn_out, batch_first=True)\n",
    "        \n",
    "        idx_last_hidden = (x_length - 1).view(-1, 1).expand(len(x_length), x_unpacked.size(2)).unsqueeze(1)\n",
    "        idx_last_hidden = idx_last_hidden.to(x.device)\n",
    "        x_last_hiddens = x_unpacked.gather(1, idx_last_hidden).squeeze(1)\n",
    "        \n",
    "        attention_out = self.attention(x_unpacked, x_last_hiddens)\n",
    "        x = self.head(attention_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, NUM_LAYERS, INPUT_SIZE, HIDDEN_SIZE, LINEAR_SIZE, OUTPUT_SIZE, DROPOUT):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.linear_size = LINEAR_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.dropout = DROPOUT\n",
    "        \n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                        d_model=self.input_size, \n",
    "                        nhead=4, \n",
    "                        dim_feedforward=self.hidden_size, \n",
    "                        dropout=self.dropout, \n",
    "                        activation='relu'\n",
    "                )\n",
    "        \n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "                        encoder_layer=transformer_encoder_layer, \n",
    "                        num_layers=self.num_layers, \n",
    "                        norm=None\n",
    "                )\n",
    "        \n",
    "        self.weighted_mean = nn.Conv1d(\n",
    "                        in_channels=self.input_size, \n",
    "                        out_channels=self.input_size, \n",
    "                        kernel_size=100, \n",
    "                        groups=self.input_size)\n",
    "    \n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(in_features=52, out_features=self.output_size),\n",
    "#                         nn.Linear(in_features=52, out_features=self.linear_size),\n",
    "#                         nn.ReLU(),\n",
    "#                         nn.Dropout(p=0.4),\n",
    "#                         nn.Linear(in_features=self.linear_size, out_features=self.output_size),\n",
    "                )\n",
    "        \n",
    "    \n",
    "    def get_params(self):\n",
    "        \n",
    "        return {\n",
    "                \"HIDDEN_SIZE\": self.hidden_size,\n",
    "                \"NUM_LAYERS\": self.num_layers,\n",
    "                \"INPUT_SIZE\": self.input_size,\n",
    "                \"LINEAR_SIZE\": self.linear_size,\n",
    "                \"OUTPUT_SIZE\": self.output_size,\n",
    "                \"DROPOUT\": self.dropout\n",
    "            }    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, x_length=None):\n",
    "        \"\"\"\n",
    "        src: (S, N, E) = (sequence_length, batch_size, n_features)\n",
    "        src_key_padding_mask: (N, S) = (batch_size, sequence_length)\n",
    "        \"\"\"\n",
    "    \n",
    "        x_mask = torch.zeros(x.size(0), x.size(1), dtype=bool, device=x.device)\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            x_mask[i, x_length[i]:] = True\n",
    "\n",
    "        x = self.transformer_encoder(src=x.transpose(0, 1), src_key_padding_mask=x_mask)\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.weighted_mean(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_SIZE = 128\n",
    "LINEAR_SIZE = 64\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "RNN_TYPE = \"LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# model = UniRNN(\n",
    "#             RNN_TYPE=RNN_TYPE, NUM_LAYERS=NUM_LAYERS, INPUT_SIZE=52, HIDDEN_SIZE=HIDDEN_SIZE, \n",
    "#             LINEAR_SIZE=LINEAR_SIZE, OUTPUT_SIZE=NUM_CLASSES, BIDIRECTIONAL=BIDIRECTIONAL, \n",
    "#             DESCRIPTION='simple_model_for_metrics'\n",
    "#         )\n",
    "\n",
    "model = AttentionModel(\n",
    "            RNN_TYPE=RNN_TYPE, NUM_LAYERS=NUM_LAYERS, INPUT_SIZE=52, HIDDEN_SIZE=HIDDEN_SIZE, \n",
    "            LINEAR_SIZE=LINEAR_SIZE, OUTPUT_SIZE=NUM_CLASSES, BIDIRECTIONAL=BIDIRECTIONAL\n",
    "        )\n",
    "\n",
    "# model = TransformerModel(\n",
    "#             NUM_LAYERS=6, INPUT_SIZE=52, HIDDEN_SIZE=128, LINEAR_SIZE=52, OUTPUT_SIZE=21, DROPOUT=0.4\n",
    "#         )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# scheduler = StepLR(optimizer, step_size=25, gamma=0.5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "64 64 64\n",
      "tensor([250, 250, 250, 250, 250, 250, 250, 250, 250, 250, 100, 100, 100, 100,\n",
      "        100, 100, 100,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,  50,\n",
      "         50,  50,  50,  25,  25,  25,  25,  25,  25,  25,  25,  25,  25,  25,\n",
      "          5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
      "          5,   5,   5,   5,   5,   5,   5,   5])\n",
      "y_batch_train.size() torch.Size([64])\n",
      "y_pred_train.size() torch.Size([64, 21]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (X_batch, X_lengths, y_batch) in enumerate(train_dl):\n",
    "    if i < 1:\n",
    "        print(type(X_batch), type(X_lengths), type(y_batch))\n",
    "        print(len(X_batch), len(X_lengths), len(y_batch))\n",
    "        print(X_lengths)\n",
    "        X_batch, y_batch_train = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred_train = model(X_batch, X_lengths)\n",
    "        print(\"y_batch_train.size()\", y_batch.size())\n",
    "        print(\"y_pred_train.size()\", y_pred_train.size(), '\\n')\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"final_models/attention/{RNN_TYPE}_{datetime.today().strftime('%d%b-%H-%M')}\"\n",
    "\n",
    "writer = SummaryWriter(log_dir=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mattention\u001b[0m/  \u001b[01;34mmulti_rnn\u001b[0m/  \u001b[01;34mrnn\u001b[0m/  \u001b[01;34mtransformer\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls final_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8cd36aa22149968de9500fde93cc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6623566101714bae9327dc0bc6e6d132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 299.6898076534271\n",
      "mean loss train: 1.059651081039792, mean loss val: 0.6743559480848766\n",
      "accuracy train: 0.6378333333333334, accuracy val: 0.7483333333333333\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 1, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797b1a8fe68443cea407e2151f6dbeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ea8a97000d46279d08dacb63d52afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 298.35732793807983\n",
      "mean loss train: 0.6661464047999609, mean loss val: 0.601646192028409\n",
      "accuracy train: 0.7514880952380952, accuracy val: 0.7726666666666666\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 2, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982225b6608c4b8493a9ea88d55a904e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3894f5f20274fbf93ab02631cba06b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 311.0398540496826\n",
      "mean loss train: 0.6025797624133882, mean loss val: 0.5469092061576389\n",
      "accuracy train: 0.7745595238095239, accuracy val: 0.802\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 3, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed4d47f230940a79f36bbd4ee20ec1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a052afbcf944679e842eee19faaba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 308.0378575325012\n",
      "mean loss train: 0.5483433213233948, mean loss val: 0.5038959453787123\n",
      "accuracy train: 0.7982261904761905, accuracy val: 0.8145238095238095\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 4, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29ffc852c0349e090ef1fd4d6f5025d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4e9e08482b4fab9d39c597ff956d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 296.4793610572815\n",
      "mean loss train: 0.4952621600627899, mean loss val: 0.47677167883373445\n",
      "accuracy train: 0.8195476190476191, accuracy val: 0.8277619047619048\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 5, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f423814ad8ca469784f6e2677dd82240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b17da5de1d41228edc8212b0797136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 300.77732038497925\n",
      "mean loss train: 0.44581863796427135, mean loss val: 0.434365860303243\n",
      "accuracy train: 0.8408333333333333, accuracy val: 0.8416666666666667\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 6, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddf5f668d4a4772b42a7e4f5d59eb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c56e860861475ebcc0863f20d98109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 298.99090027809143\n",
      "mean loss train: 0.42098990402902875, mean loss val: 0.3933875514439174\n",
      "accuracy train: 0.8488095238095238, accuracy val: 0.8538571428571429\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 7, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af79dd40b06f48988a23080399b03c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce469b9dd3454bc1a897a01cd6e98ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 298.170512676239\n",
      "mean loss train: 0.4093008270717802, mean loss val: 0.4027985534213838\n",
      "accuracy train: 0.8501190476190477, accuracy val: 0.855904761904762\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 8, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9736504b7cc4496daa0cc94c9a18e5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b81328bd49447287a23bd5dda11b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 314.76520919799805\n",
      "mean loss train: 0.38986413577057066, mean loss val: 0.3777251381419954\n",
      "accuracy train: 0.8560714285714286, accuracy val: 0.8577142857142858\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 9, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a91086cbb244210a663db7a2a6f36bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8aabe234c4429abaab8a581d661f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 366.3289077281952\n",
      "mean loss train: 0.3807320175341197, mean loss val: 0.3690131391797747\n",
      "accuracy train: 0.8591190476190477, accuracy val: 0.8604285714285714\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 10, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8dc03c59bb4e9ca19a7036c460c070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c07fc35297d4d4fbc2552eca0bc6b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 367.7108826637268\n",
      "mean loss train: 0.3756830763419469, mean loss val: 0.365535318896884\n",
      "accuracy train: 0.8596428571428572, accuracy val: 0.8607142857142858\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 11, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ee300136b54b4d959e37bb341729d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb076de41f424520892bab6eea336d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 368.2658269405365\n",
      "mean loss train: 0.3645193519336837, mean loss val: 0.3698241573288327\n",
      "accuracy train: 0.8617857142857143, accuracy val: 0.8601428571428571\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 12, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1779983135b24b7099866fe3760bfada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d919865556154abb9f739e61121c4c76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 369.1126763820648\n",
      "mean loss train: 0.3567302758977527, mean loss val: 0.36315026052792865\n",
      "accuracy train: 0.8640119047619048, accuracy val: 0.8618095238095238\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 13, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6993cdf766946ad8f7fb6135a97d7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8277c132694b10b8e301c69d9184ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 329.47579050064087\n",
      "mean loss train: 0.3473568948138328, mean loss val: 0.36100308391026087\n",
      "accuracy train: 0.8651309523809524, accuracy val: 0.8634761904761905\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 14, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7d3edd80ed468f8c18ea9c9e44c21c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89aa9c0499e451d934e92a8cbbdad9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 296.63483214378357\n",
      "mean loss train: 0.3420893178979556, mean loss val: 0.3617420968555269\n",
      "accuracy train: 0.8671666666666666, accuracy val: 0.8626666666666667\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 15, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e411a70f2644ed28894044ed2d11491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0580289702479e856b9ba3ec79b60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 299.06006145477295\n",
      "mean loss train: 0.33549763917355313, mean loss val: 0.3607689826403345\n",
      "accuracy train: 0.8695833333333334, accuracy val: 0.8633333333333333\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 16, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0ae208c5144573ba277d81403b4ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4aaf4aa5af64f00985263501a0a94bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 306.3064761161804\n",
      "mean loss train: 0.3292599746897107, mean loss val: 0.3497261465390523\n",
      "accuracy train: 0.8714880952380952, accuracy val: 0.8690476190476191\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 17, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4652b68d675a42ca9a469a8f5e9f3024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01126dc68ea441f0a8372cfac285025d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 309.672691822052\n",
      "mean loss train: 0.3288150268622807, mean loss val: 0.33378895731199354\n",
      "accuracy train: 0.8737380952380952, accuracy val: 0.8730476190476191\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 18, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c44dacd064489d91a9e8eba227848e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b1453a02747f297da049b36d24ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 370.32045245170593\n",
      "mean loss train: 0.31750127944492157, mean loss val: 0.3517050718125843\n",
      "accuracy train: 0.8790119047619047, accuracy val: 0.8686666666666667\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 19, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8302976b3743e7aceee53e3c6e6e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcf58c191bd440f82ce5c1fe9c3f8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 370.76130747795105\n",
      "mean loss train: 0.3139397267727625, mean loss val: 0.34235169499261037\n",
      "accuracy train: 0.8798571428571429, accuracy val: 0.8670952380952381\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 20, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ad29395ce8494dbc92b77aa6a29065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b7c765e2834510801703bc46f0ab8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 365.4610471725464\n",
      "mean loss train: 0.30802278277987527, mean loss val: 0.34280290184702195\n",
      "accuracy train: 0.8821785714285715, accuracy val: 0.8729047619047619\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 21, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8a805f31e54b1c894d03c3ea375383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737db85e4fce41fc8c043fe0989cd2d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 362.447895526886\n",
      "mean loss train: 0.3056751144187791, mean loss val: 0.33965987725484936\n",
      "accuracy train: 0.8843214285714286, accuracy val: 0.8733809523809524\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 22, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecbd9ab09ea4128b9971bfccd084b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a2fe213b34458fa7d3188a207bb3e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 479.63812279701233\n",
      "mean loss train: 0.29871574514252797, mean loss val: 0.3393664341881162\n",
      "accuracy train: 0.8881428571428571, accuracy val: 0.8728571428571429\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 23, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3862420713e4234bf09ffb5dfe73d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81868cac1fc455cb41fce400807468c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 364.24552631378174\n",
      "mean loss train: 0.29426091324147724, mean loss val: 0.34348308553014484\n",
      "accuracy train: 0.8904404761904762, accuracy val: 0.8685714285714285\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 24, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000815df10304455aefc0d00b1ed3173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e119ffc9dc4f9ab1cff0185031dd35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 360.2420611381531\n",
      "mean loss train: 0.28892091769263856, mean loss val: 0.34541437558900745\n",
      "accuracy train: 0.8923571428571428, accuracy val: 0.8724285714285714\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 25, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cb7f057bd94cdd97968428b4875c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24d540b4f60413398124985327957fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=83.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch time: 362.51913237571716\n",
      "mean loss train: 0.2842370208388283, mean loss val: 0.33401248050303683\n",
      "accuracy train: 0.8951071428571429, accuracy val: 0.8715238095238095\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Epoch: 26, Learning Rate: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef433b3aae04b2ea2c81e32795c95f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_train_all, loss_val_all = [], []\n",
    "accuracy_train_all, accuracy_val_all = [], []\n",
    "\n",
    "best_model_acc = 0\n",
    "best_model_epoch = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    start = time.time()\n",
    "    print(f\"Epoch: {epoch}, Learning Rate: {optimizer.param_groups[0]['lr']}\\n\")\n",
    "\n",
    "    loss_train_epoch, loss_val_epoch = 0, 0\n",
    "    correct_train_epoch, correct_val_epoch = 0, 0\n",
    "    n_train, n_val = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    for (X_batch_train, X_batch_lengths_train, y_batch_train) in tqdm(train_dl):\n",
    "\n",
    "        X_batch_train, X_batch_lengths_train, y_batch_train =\\\n",
    "                    X_batch_train.to(device), X_batch_lengths_train.to(device), y_batch_train.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_pred_train = model(X_batch_train, X_batch_lengths_train)\n",
    "        loss_train = criterion(y_pred_train, y_batch_train)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train_epoch += loss_train.item() * y_batch_train.size()[0]\n",
    "        correct_train_epoch += correct(y_pred_train, y_batch_train)\n",
    "        n_train += y_batch_train.size()[0]\n",
    "\n",
    "#     scheduler.step()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (X_batch_val, X_batch_lengths_val, y_batch_val) in tqdm(val_dl):\n",
    "\n",
    "            X_batch_val, X_batch_lengths_val, y_batch_val =\\\n",
    "                    X_batch_val.to(device), X_batch_lengths_val.to(device), y_batch_val.to(device)\n",
    "\n",
    "            y_pred_val = model(X_batch_val, X_batch_lengths_val)\n",
    "            loss_val = criterion(y_pred_val, y_batch_val)\n",
    "            \n",
    "            loss_val_epoch += loss_val.item() * y_batch_val.size()[0]\n",
    "            correct_val_epoch += correct(y_pred_val, y_batch_val)\n",
    "            n_val += y_batch_val.size()[0]\n",
    "            \n",
    "    loss_mean_train_epoch = loss_train_epoch / n_train\n",
    "    loss_mean_val_epoch = loss_val_epoch / n_val\n",
    "\n",
    "    loss_train_all.append(loss_mean_train_epoch)\n",
    "    loss_val_all.append(loss_mean_val_epoch)\n",
    "\n",
    "    accuracy_train_epoch = correct_train_epoch / n_train\n",
    "    accuracy_val_epoch = correct_val_epoch / n_val\n",
    "\n",
    "    accuracy_train_all.append(accuracy_train_epoch)\n",
    "    accuracy_val_all.append(accuracy_val_epoch)\n",
    "\n",
    "    writer.add_scalars('LOSS per epoch', {\"train\": loss_mean_train_epoch, \"val\": loss_mean_val_epoch}, epoch)\n",
    "    writer.add_scalars('ACCURACY per epoch', {\"train\": accuracy_train_epoch, \"val\": accuracy_val_epoch}, epoch)\n",
    "    \n",
    "    if accuracy_val_epoch > best_model_acc:\n",
    "        \n",
    "        best_model_state_dict = model.state_dict()\n",
    "        best_model_acc = accuracy_val_epoch\n",
    "        best_model_epoch = epoch\n",
    "        \n",
    "    scheduler.step(loss_mean_val_epoch)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"epoch time: {end - start}\")  \n",
    "    print(f\"mean loss train: {loss_mean_train_epoch}, mean loss val: {loss_mean_val_epoch}\")\n",
    "    print(f\"accuracy train: {accuracy_train_epoch}, accuracy val: {accuracy_val_epoch}\")\n",
    "\n",
    "    print(\"---------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.add_hparams(model.get_params(), {\"best_accuracy\": best_model_acc, \"best_model_epoch\": best_model_epoch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Loading best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_name.replace(\"final_models/\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls final_saved_models/attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'model': model.__class__.__name__, \n",
    "              'params': model.get_params(),\n",
    "              'state_dict': best_model_state_dict}\n",
    "\n",
    "torch.save(checkpoint, f\"final_saved_models/{model_name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls final_saved_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f\"final_saved_models/{model_name}.pth\")\n",
    "\n",
    "model = getattr(sys.modules[__name__], checkpoint['model'])(**checkpoint['params'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Val evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates TPR, FPR and ACCURACY per class for multiple simulation runs\n",
    "    https://stackoverflow.com/questions/50666091/true-positive-rate-and-false-positive-rate-tpr-fpr-for-multi-class-data-in-py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : \n",
    "        type: np.array \n",
    "        shape : (number of simulation runs)\n",
    "        description: true classes for simulation runs\n",
    "    \n",
    "    y_pred : np.array\n",
    "    \n",
    "        type: np.array \n",
    "        shape : (number of simulation runs)\n",
    "        description: predicted classes for simulation runs\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    TPR : \n",
    "        type: list of floats\n",
    "        shape: (number of classes)\n",
    "        description: True Positive Rate per class\n",
    "    FPR : \n",
    "        type: list of floats\n",
    "        shape: (number of classes)\n",
    "        description: False Positive Rate per class\n",
    "    ACCURACY : \n",
    "        type: list of floats\n",
    "        shape: (number of classes)\n",
    "        description: Accuracy \"one vs all\" per class\n",
    "    \"\"\"\n",
    "    \n",
    "    conf = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    FP = conf.sum(axis=0) - np.diag(conf)\n",
    "    FN = conf.sum(axis=1) - np.diag(conf)\n",
    "    TP = np.diag(conf)\n",
    "    TN = conf.sum() - (FP + FN + TP)\n",
    "    \n",
    "    TPR = TP / (TP + FN)\n",
    "    FPR = FP / (FP + TN)\n",
    "\n",
    "    ACCURACY = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    return TPR, FPR, ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_true_idx(arr):\n",
    "    idx = np.where(arr == True)[0]\n",
    "    if len(idx) == 0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return idx.min()\n",
    "\n",
    "def get_detection_delay(y_true, y_pred) -> dict():\n",
    "    \"\"\"\n",
    "    Calculates detection delay for every simulation run\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : \n",
    "        type : np.array \n",
    "        shape : (number of simulation runs)\n",
    "        description : true classes for simulation runs\n",
    "    \n",
    "    y_pred : np.array \n",
    "        type : np.array \n",
    "        shape : (number of simulation runs, simulation runs' lengths)\n",
    "        description: predicted classes for every sample for every simulation runs\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    detection_delay :\n",
    "        type : dict\n",
    "        keys : classes from 0 to 20\n",
    "        description : dict of detection delays for every class, nan means true class wasn't predicted\n",
    "    \n",
    "    Commentary:\n",
    "        If you want to get avarage detection delays per class, you need to calulate avg of detection_delay[key]\n",
    "        for every key \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    detection_delay = defaultdict(list)\n",
    "    \n",
    "    correct = y_pred == y_true[..., np.newaxis]\n",
    "    first_true_idxs = np.apply_along_axis(func1d=get_first_true_idx, arr=correct, axis=1)\n",
    "\n",
    "    for (cls, idx) in zip(y_true, first_true_idxs):\n",
    "        detection_delay[cls].append(idx)\n",
    "    return detection_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "y_ans_val, y_true_val = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for (X_batch_val, X_batch_lengths_val, y_batch_val) in tqdm(val_dl):\n",
    "\n",
    "        X_batch_val, X_batch_lengths_val, y_batch_val =\\\n",
    "                    X_batch_val.to(device), X_batch_lengths_val.to(device), y_batch_val.to(device)\n",
    "\n",
    "        y_pred_val = model(X_batch_val, X_batch_lengths_val)\n",
    "        \n",
    "        y_pred_prob = F.softmax(y_pred_val.cpu(), dim=-1)\n",
    "        y_pred_class = y_pred_prob.max(dim=-1)[1]\n",
    "       \n",
    "        y_ans_val += y_pred_class.tolist()\n",
    "        y_true_val += y_batch_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 8))\n",
    "# plt.title(\"loss\")\n",
    "# plt.plot(np.arange(len(loss_train_all)), loss_train_all, '-o', marker='.', label='train')\n",
    "# plt.plot(np.arange(len(loss_val_all)), loss_val_all, '-o', marker='.', label='val')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 8))\n",
    "# plt.title(\"accuracy\")\n",
    "# plt.plot(np.arange(len(accuracy_train_all)), accuracy_train_all, '-o', marker='.', label='train')\n",
    "# plt.plot(np.arange(len(accuracy_val_all)), accuracy_val_all, '-o', marker='.', label='val')\n",
    "# plt.text(35, 0.65, s=f\"best acc: {best_model_acc}\\nbest epoch: {best_model_epoch}\", fontsize=20)\n",
    "# plt.scatter(best_model_epoch, best_model_acc, c='g', s=100)\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.title(\"FDR\")\n",
    "sns.heatmap(confusion_matrix(y_true_val, y_ans_val, normalize='pred'), annot=True, cmap=sns.cm.rocket_r)\n",
    "plt.xlabel('predicted class')\n",
    "plt.ylabel('true class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR, FPR, ACCURACY = get_metrics(y_true_val, y_ans_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(arr, name, forward=True):\n",
    "    classes = np.arange(len(arr))\n",
    "\n",
    "    norm = plt.Normalize(arr.min(), arr.max())\n",
    "    \n",
    "    if forward:\n",
    "        colors = plt.cm.RdYlGn(norm(arr))\n",
    "    else:\n",
    "        colors = plt.cm.summer(norm(arr))\n",
    "        \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.title(f'{name} per class')\n",
    "#     sns.barplot(x=classes, y=arr, palette=colors)\n",
    "    plt.plot(arr, '-*', color='yellowgreen')\n",
    "    plt.xticks(classes, [\"fault_\" + str(c) if c > 0 else \"normal\" for c in classes], rotation=90)\n",
    "    plt.xlabel('class')\n",
    "    plt.ylabel(f'{name}')\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting(TPR, \"TPR (true positive rate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting(FPR, \"FPR (false positive rate)\", forward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting(ACCURACY, \"ACCURACY vs ALL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading test data in .R format\n",
    "a3 = py.read_r(\"../../data/raw/dataverse_files/TEP_FaultFree_Testing.RData\")\n",
    "a4 = py.read_r(\"../../data/raw/dataverse_files/TEP_Faulty_Testing.RData\")\n",
    "\n",
    "raw_test = pd.concat([a3['fault_free_testing'], a4['faulty_testing']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test[features] = scaler.transform(raw_test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test['index'] = raw_test['faultNumber'] * 500 + raw_test['simulationRun'] - 1\n",
    "raw_test = raw_test.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTEST(Dataset):\n",
    "\n",
    "    def __init__(self, X, seq_length):\n",
    "    \n",
    "        self.X = X\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.features = [\n",
    "                'xmeas_1', 'xmeas_2', 'xmeas_3', 'xmeas_4', 'xmeas_5', 'xmeas_6', 'xmeas_7', 'xmeas_8', 'xmeas_9', \n",
    "                'xmeas_10', 'xmeas_11', 'xmeas_12', 'xmeas_13', 'xmeas_14', 'xmeas_15', 'xmeas_16', 'xmeas_17', \n",
    "                'xmeas_18', 'xmeas_19', 'xmeas_20', 'xmeas_21', 'xmeas_22', 'xmeas_23', 'xmeas_24', 'xmeas_25', \n",
    "                'xmeas_26', 'xmeas_27', 'xmeas_28', 'xmeas_29', 'xmeas_30', 'xmeas_31', 'xmeas_32', 'xmeas_33', \n",
    "                'xmeas_34', 'xmeas_35', 'xmeas_36', 'xmeas_37', 'xmeas_38', 'xmeas_39', 'xmeas_40', 'xmeas_41', \n",
    "                'xmv_1', 'xmv_2', 'xmv_3', 'xmv_4', 'xmv_5', 'xmv_6', 'xmv_7', 'xmv_8', 'xmv_9', 'xmv_10', 'xmv_11'\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.index.nunique()\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        features = self.X.loc[idx][self.features].values[160 : (161+self.seq_length), :]\n",
    "        target = self.X.loc[idx]['faultNumber'].unique()[0]\n",
    "\n",
    "        features = torch.tensor(features, dtype=torch.float)\n",
    "        target = torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SEQ_LENGTH = [1, 5, 10, 25, 50, 100, 200, 250, 300]\n",
    "metrics = dict()\n",
    "y_ans_test_all = []\n",
    "\n",
    "for seq_length in SEQ_LENGTH:\n",
    "    \n",
    "    y_ans_test, y_true_test = [], []\n",
    "    \n",
    "    test_ds = DataTEST(raw_test, seq_length=seq_length)\n",
    "    test_dl = DataLoader(test_ds, batch_size=512)\n",
    "\n",
    "    start = time.time()\n",
    "    print(f'seq_length: {seq_length}\\n')\n",
    "\n",
    "    model.eval()\n",
    "    for (X_batch_test, y_batch_test) in tqdm(test_dl):\n",
    "        \n",
    "        print(X_batch_test.size())\n",
    "        \n",
    "        X_batch_lengths_test = torch.tensor([seq_length]*len(X_batch_test)).to(device)\n",
    "\n",
    "        X_batch_test, y_batch_test = X_batch_test.to(device), y_batch_test.to(device)\n",
    "\n",
    "        y_pred_test = model(X_batch_test, X_batch_lengths_test)\n",
    "\n",
    "        y_pred_prob = F.softmax(y_pred_test.cpu(), dim=-1)\n",
    "        y_pred_class = y_pred_prob.max(dim=-1)[1]\n",
    "       \n",
    "        y_ans_test += y_pred_class.tolist()\n",
    "        y_true_test += y_batch_test.tolist()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "    y_ans_test_all.append(y_ans_test)\n",
    "        \n",
    "    TPR, FPR, ACCURACY = get_metrics(y_true_test, y_ans_test)\n",
    "    \n",
    "#     print(\"Classes True\", round(pd.Series(y_true_test).value_counts(normalize=True).sort_index(), 4).to_dict())\n",
    "#     print(\"Classes False\", round(pd.Series(y_ans_test).value_counts(normalize=True).sort_index(), 4).to_dict())\n",
    "    \n",
    "#     print(\"TPR\", np.round(TPR, 4))\n",
    "#     print(\"FPR\", np.round(FPR, 4))\n",
    "#     print(\"ACCURACY\", np.round(ACCURACY, 4))\n",
    "    \n",
    "    metrics[seq_length] = [TPR, FPR, ACCURACY]\n",
    "    \n",
    "    print(f\"seq_length time: {end - start}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ans_test_all = np.array(y_ans_test_all).T\n",
    "y_true_test_all = np.array(y_true_test).T\n",
    "\n",
    "y_ans_test_all.shape, y_true_test_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_delay = get_detection_delay(y_true=y_true_test_all, y_pred=y_ans_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting(metrics[seq_length][0], \"TPR\")\n",
    "plotting(metrics[seq_length][1], \"FPR\")\n",
    "plotting(metrics[seq_length][2], \"ACCURACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_delay_mean = {}\n",
    "for (k, v) in detection_delay.items():\n",
    "    detection_delay_mean[k] = np.nanmean(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_delay_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(list(detection_delay_mean.values())), '-go')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwinModel(torch.nn.Module) :\n",
    "    def __init__(self, NUM_LAYERS, INPUT_SIZE, HIDDEN_SIZE, LINEAR_SIZE, OUTPUT_SIZE, BIDIRECTIONAL, DEVICE):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.linear_size = LINEAR_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.bidirectional = BIDIRECTIONAL\n",
    "        \n",
    "        self.lstm_1 = nn.LSTM(\n",
    "                        input_size=self.input_size[0], \n",
    "                        hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        batch_first=True,\n",
    "                        dropout=0.4\n",
    "            )\n",
    "        \n",
    "        self.lstm_2 = nn.LSTM(\n",
    "                        input_size=self.input_size[1],\n",
    "                        hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        batch_first=True,\n",
    "                        dropout=0.4\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Linear(in_features=2*self.hidden_size*(self.bidirectional+1), out_features=self.linear_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(in_features=self.linear_size, out_features=OUTPUT_SIZE),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x_1 = x[:, :, :41]\n",
    "        x_2 = x[:, :, 41:]\n",
    "        \n",
    "        x_1, _ = self.lstm_1(x_1)\n",
    "        x_2, __ = self.lstm_2(x_2)\n",
    "        \n",
    "        x_3 = torch.cat((x_1[:, -1], x_2[:, -1]), dim=-1)\n",
    "        \n",
    "        x = self.head(x_3)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
