{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pyreadr as py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.optim import Adam, RMSprop\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "# device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Data downloading\n",
    "Data link  \n",
    "https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/6C3JR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data description\n",
    "Here we consoder dataset of \"Additional Tennessee Eastman Process Simulation Data for Anomaly Detection Evaluation\"\n",
    "This dataverse contains the data referenced in Rieth et al. (2017). Issues and Advances in Anomaly Detection Evaluation for Joint Human-Automated Systems. To be presented at Applied Human Factors and Ergonomics 2017.\n",
    "##### Columns description\n",
    "* **faultNumber** ranges from 1 to 20 in the “Faulty” datasets and represents the fault type in the TEP. The “FaultFree” datasets only contain fault 0 (i.e. normal operating conditions).\n",
    "* **simulationRun** ranges from 1 to 500 and represents a different random number generator state from which a full TEP dataset was generated (Note: the actual seeds used to generate training and testing datasets were non-overlapping).\n",
    "* **sample** ranges either from 1 to 500 (“Training” datasets) or 1 to 960 (“Testing” datasets). The TEP variables (columns 4 to 55) were sampled every 3 minutes for a total duration of 25 hours and 48 hours respectively. Note that the faults were introduced 1 and 8 hours into the Faulty Training and Faulty Testing datasets, respectively.\n",
    "* **columns 4-55** contain the process variables; the column names retain the original variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniRNN(nn.Module) :\n",
    "    def __init__(self, RNN_TYPE, NUM_LAYERS, INPUT_SIZE, HIDDEN_SIZE, LINEAR_SIZE, OUTPUT_SIZE, BIDIRECTIONAL, DESCRIPTION):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn_type = RNN_TYPE\n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.linear_size = LINEAR_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.bidirectional = BIDIRECTIONAL\n",
    "        self.description = DESCRIPTION\n",
    "        \n",
    "        rnn_cell = getattr(nn, RNN_TYPE)\n",
    "        \n",
    "        self.rnn = rnn_cell(\n",
    "                        input_size=self.input_size, \n",
    "                        hidden_size=self.hidden_size,\n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        batch_first=True,\n",
    "                        dropout=0.4\n",
    "                )    \n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Linear(in_features=self.hidden_size*(self.bidirectional+1), out_features=self.linear_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(in_features=self.linear_size, out_features=self.output_size),\n",
    "                )\n",
    "    \n",
    "    def get_params(self):\n",
    "        \n",
    "        return {\n",
    "            \"RNN_TYPE\": self.rnn_type,\n",
    "            \"HIDDEN_SIZE\": self.hidden_size,\n",
    "            \"NUM_LAYERS\": self.num_layers,\n",
    "            \"INPUT_SIZE\": self.input_size,\n",
    "            \"LINEAR_SIZE\": self.linear_size,\n",
    "            \"OUTPUT_SIZE\": self.output_size,\n",
    "            \"BIDIRECTIONAL\": self.bidirectional,\n",
    "            \"DESCRIPTION\": self.description,\n",
    "            }\n",
    "            \n",
    "    def forward(self, x, x_length):\n",
    "        \n",
    "        print(\"x_length\", x_length.size())\n",
    "        \n",
    "        x_length = \n",
    "        \n",
    "        x_packed = pack_padded_sequence(x, x_length, batch_first=True)\n",
    "        x_rnn_out, _ = self.rnn(x_packed)\n",
    "        x_unpacked, _ = pad_packed_sequence(x_rnn_out, batch_first=True)\n",
    "        \n",
    "        idx_last_hidden = (x_length - 1).view(-1, 1).expand(len(x_length), x_unpacked.size(2)).unsqueeze(1)\n",
    "        idx_last_hidden = idx_last_hidden.to(x.device)\n",
    "        x_last_hiddens = x_unpacked.gather(1, idx_last_hidden).squeeze(1)\n",
    "        \n",
    "        x = self.head(x_last_hiddens)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(1, 500, (3,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(self, RNN_TYPE, NUM_LAYERS, INPUT_SIZE, HIDDEN_SIZE, LINEAR_SIZE, OUTPUT_SIZE, BIDIRECTIONAL):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.rnn_type = RNN_TYPE\n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.linear_size = LINEAR_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.bidirectional = BIDIRECTIONAL\n",
    "        \n",
    "        rnn_cell = getattr(nn, RNN_TYPE)\n",
    "\n",
    "        self.rnn = rnn_cell(\n",
    "                        input_size=self.input_size, \n",
    "                        hidden_size=self.hidden_size, \n",
    "                        num_layers=self.num_layers, \n",
    "                        bidirectional=self.bidirectional,\n",
    "                        dropout=0.4,\n",
    "                        batch_first=True\n",
    "                )\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "#                         nn.Linear(in_features=self.hidden_size*(self.bidirectional+1), out_features=self.output_size),\n",
    "                        nn.Linear(in_features=self.hidden_size*(self.bidirectional+1), out_features=self.linear_size),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(p=0.4),\n",
    "                        nn.Linear(in_features=self.linear_size, out_features=self.output_size),\n",
    "                )\n",
    "        \n",
    "        \n",
    "    def get_params(self):\n",
    "        \n",
    "        return {\n",
    "            \"RNN_TYPE\": self.rnn_type, \n",
    "            \"HIDDEN_SIZE\": self.hidden_size,\n",
    "            \"NUM_LAYERS\": self.num_layers,\n",
    "            \"INPUT_SIZE\": self.input_size,\n",
    "            \"LINEAR_SIZE\": self.linear_size,\n",
    "            \"OUTPUT_SIZE\": self.output_size,\n",
    "            \"BIDIRECTIONAL\": self.bidirectional\n",
    "        }\n",
    "    \n",
    "\n",
    "    def attention(self, lstm_output, last_hidden):\n",
    "        \n",
    "        attn_weights = torch.bmm(lstm_output, last_hidden.unsqueeze(2)).squeeze(2)\n",
    "        soft_attn_weights = F.softmax(attn_weights, dim=1)\n",
    "        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return new_hidden_state\n",
    "    \n",
    "    def forward(self, x, x_length):\n",
    "\n",
    "        x_packed = pack_padded_sequence(x, x_length, batch_first=True)\n",
    "        \n",
    "        x_rnn_out, _ = self.rnn(x_packed)\n",
    "        \n",
    "        x_unpacked, __ = pad_packed_sequence(x_rnn_out, batch_first=True)\n",
    "        \n",
    "        idx_last_hidden = (x_length - 1).view(-1, 1).expand(len(x_length), x_unpacked.size(2)).unsqueeze(1)\n",
    "        idx_last_hidden = idx_last_hidden.to(x.device)\n",
    "        x_last_hiddens = x_unpacked.gather(1, idx_last_hidden).squeeze(1)\n",
    "        \n",
    "        attention_out = self.attention(x_unpacked, x_last_hiddens)\n",
    "        x = self.head(attention_out)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(torch.nn.Module):\n",
    "    def __init__(self, NUM_LAYERS, INPUT_SIZE, HIDDEN_SIZE, LINEAR_SIZE, OUTPUT_SIZE, DROPOUT):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = HIDDEN_SIZE\n",
    "        self.num_layers = NUM_LAYERS\n",
    "        self.input_size = INPUT_SIZE\n",
    "        self.linear_size = LINEAR_SIZE\n",
    "        self.output_size = OUTPUT_SIZE\n",
    "        self.dropout = DROPOUT\n",
    "        \n",
    "        transformer_encoder_layer = nn.TransformerEncoderLayer(\n",
    "                        d_model=self.input_size, \n",
    "                        nhead=4, \n",
    "                        dim_feedforward=self.hidden_size, \n",
    "                        dropout=self.dropout, \n",
    "                        activation='relu'\n",
    "                )\n",
    "        \n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "                        encoder_layer=transformer_encoder_layer, \n",
    "                        num_layers=self.num_layers, \n",
    "                        norm=None\n",
    "                )\n",
    "        \n",
    "        self.weighted_mean = nn.Conv1d(\n",
    "                        in_channels=self.input_size, \n",
    "                        out_channels=self.input_size, \n",
    "                        kernel_size=100, \n",
    "                        groups=self.input_size)\n",
    "    \n",
    "        self.head = nn.Sequential(\n",
    "                        nn.Dropout(p=0.2),\n",
    "                        nn.Linear(in_features=52, out_features=self.output_size),\n",
    "#                         nn.Linear(in_features=52, out_features=self.linear_size),\n",
    "#                         nn.ReLU(),\n",
    "#                         nn.Dropout(p=0.4),\n",
    "#                         nn.Linear(in_features=self.linear_size, out_features=self.output_size),\n",
    "                )\n",
    "        \n",
    "    \n",
    "    def get_params(self):\n",
    "        \n",
    "        return {\n",
    "                \"HIDDEN_SIZE\": self.hidden_size,\n",
    "                \"NUM_LAYERS\": self.num_layers,\n",
    "                \"INPUT_SIZE\": self.input_size,\n",
    "                \"LINEAR_SIZE\": self.linear_size,\n",
    "                \"OUTPUT_SIZE\": self.output_size,\n",
    "                \"DROPOUT\": self.dropout\n",
    "            }    \n",
    "    \n",
    "    \n",
    "    def forward(self, x, x_length=None):\n",
    "        \"\"\"\n",
    "        src: (S, N, E) = (sequence_length, batch_size, n_features)\n",
    "        src_key_padding_mask: (N, S) = (batch_size, sequence_length)\n",
    "        \"\"\"\n",
    "    \n",
    "        x_mask = torch.zeros(x.size(0), x.size(1), dtype=bool, device=x.device)\n",
    "        \n",
    "        for i in range(len(x)):\n",
    "            x_mask[i, x_length[i]:] = True\n",
    "\n",
    "        x = self.transformer_encoder(src=x.transpose(0, 1), src_key_padding_mask=x_mask)\n",
    "        x = x.permute(1, 2, 0)\n",
    "        x = self.weighted_mean(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_SIZE = 128\n",
    "LINEAR_SIZE = 64\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "RNN_TYPE = \"LSTM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniRNN(\n",
       "  (rnn): LSTM(52, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=21, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "model = UniRNN(\n",
    "            RNN_TYPE=RNN_TYPE, NUM_LAYERS=NUM_LAYERS, INPUT_SIZE=52, HIDDEN_SIZE=HIDDEN_SIZE, \n",
    "            LINEAR_SIZE=LINEAR_SIZE, OUTPUT_SIZE=NUM_CLASSES, BIDIRECTIONAL=BIDIRECTIONAL, \n",
    "            DESCRIPTION='simple_model_for_metrics'\n",
    "        )\n",
    "\n",
    "# model = AttentionModel(\n",
    "#             RNN_TYPE=RNN_TYPE, NUM_LAYERS=NUM_LAYERS, INPUT_SIZE=52, HIDDEN_SIZE=HIDDEN_SIZE, \n",
    "#             LINEAR_SIZE=LINEAR_SIZE, OUTPUT_SIZE=NUM_CLASSES, BIDIRECTIONAL=BIDIRECTIONAL\n",
    "#         )\n",
    "\n",
    "# model = TransformerModel(\n",
    "#             NUM_LAYERS=6, INPUT_SIZE=52, HIDDEN_SIZE=128, LINEAR_SIZE=52, OUTPUT_SIZE=21, DROPOUT=0.4\n",
    "#         )\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UniRNN(\n",
      "  (rnn): LSTM(52, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=21, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(model):\n",
    "\n",
    "    for i, item in enumerate(model.named_modules()):\n",
    "        print(i, item[1])\n",
    "        \n",
    "    print(\"------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    for i, item in enumerate(model.named_modules()):\n",
    "        if i not in [0, 2]:\n",
    "            print(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 UniRNN(\n",
      "  (rnn): LSTM(52, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (head): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.4, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=21, bias=True)\n",
      "  )\n",
      ")\n",
      "1 LSTM(52, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "2 Sequential(\n",
      "  (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.4, inplace=False)\n",
      "  (3): Linear(in_features=64, out_features=21, bias=True)\n",
      ")\n",
      "3 Linear(in_features=256, out_features=64, bias=True)\n",
      "4 ReLU()\n",
      "5 Dropout(p=0.4, inplace=False)\n",
      "6 Linear(in_features=64, out_features=21, bias=True)\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "LSTM(52, 128, num_layers=2, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "Linear(in_features=256, out_features=64, bias=True)\n",
      "ReLU()\n",
      "Dropout(p=0.4, inplace=False)\n",
      "Linear(in_features=64, out_features=21, bias=True)\n"
     ]
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
