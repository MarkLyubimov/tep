{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '_', 'a', 'c', 'd', 'e', 'g', 'i', 'm', 'n', 'r', 's', 't', 'u', 'y']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = ['gigantic_string','tiny_str','medium_str']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(set(''.join(seqs)))\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make model\n",
    "embed = nn.Embedding(len(vocab), 10).cpu()\n",
    "lstm = nn.LSTM(10, 5).cpu()\n",
    "lstm.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_seqs = [[vocab.index(tok) for tok in seq] for seq in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7, 6, 2, 9, 12, 7, 3, 1, 11, 12, 10, 7, 9, 6],\n",
       " [12, 7, 9, 14, 1, 11, 12, 10],\n",
       " [8, 5, 4, 7, 13, 8, 1, 11, 12, 10]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15,  8, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor([len(seq) for seq in vectorized_seqs]).cpu()\n",
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  6,  2,  9, 12,  7,  3,  1, 11, 12, 10,  7,  9,  6],\n",
       "        [12,  7,  9, 14,  1, 11, 12, 10,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 8,  5,  4,  7, 13,  8,  1, 11, 12, 10,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long().cpu()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    \n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  6,  2,  9, 12,  7,  3,  1, 11, 12, 10,  7,  9,  6],\n",
       "        [ 8,  5,  4,  7, 13,  8,  1, 11, 12, 10,  0,  0,  0,  0,  0],\n",
       "        [12,  7,  9, 14,  1, 11, 12, 10,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  8, 12],\n",
       "        [ 7,  5,  7],\n",
       "        [ 6,  4,  9],\n",
       "        [ 2,  7, 14],\n",
       "        [ 9, 13,  1],\n",
       "        [12,  8, 11],\n",
       "        [ 7,  1, 12],\n",
       "        [ 3, 11, 10],\n",
       "        [ 1, 12,  0],\n",
       "        [11, 10,  0],\n",
       "        [12,  0,  0],\n",
       "        [10,  0,  0],\n",
       "        [ 7,  0,  0],\n",
       "        [ 9,  0,  0],\n",
       "        [ 6,  0,  0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,\n",
       "           0.9931,  0.5774, -0.6358],\n",
       "         [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,\n",
       "           2.2093, -0.3803,  1.0325],\n",
       "         [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698]],\n",
       "\n",
       "        [[ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [-0.9049, -2.2375, -0.5936, -0.4607, -0.6410, -0.8513, -0.8477,\n",
       "           0.1328, -0.8828,  0.5129],\n",
       "         [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784]],\n",
       "\n",
       "        [[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,\n",
       "           0.9931,  0.5774, -0.6358],\n",
       "         [ 0.2574, -0.9649, -1.3667,  0.3344, -0.1496,  1.3959,  0.5439,\n",
       "           0.6314,  0.3370,  1.6015],\n",
       "         [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,\n",
       "           1.3961,  0.2879,  0.4053]],\n",
       "\n",
       "        [[ 0.4495, -0.0517,  0.3217, -1.3956, -0.8258, -0.4533, -0.6968,\n",
       "           0.1905,  0.0951, -1.3101],\n",
       "         [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [ 1.2062,  0.8103, -0.0299, -1.1283,  0.7842,  1.0114, -1.7564,\n",
       "           1.2274, -0.7476, -1.6700]],\n",
       "\n",
       "        [[-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,\n",
       "           1.3961,  0.2879,  0.4053],\n",
       "         [-1.1101, -0.9401, -1.4768,  0.3649, -0.6038,  0.9148,  1.4016,\n",
       "          -0.4053,  1.5718, -0.5589],\n",
       "         [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151,\n",
       "          -0.2171, -0.3291,  0.5413]],\n",
       "\n",
       "        [[ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698],\n",
       "         [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,\n",
       "           2.2093, -0.3803,  1.0325],\n",
       "         [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155,\n",
       "          -0.3412,  0.0888,  0.4907]],\n",
       "\n",
       "        [[ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151,\n",
       "          -0.2171, -0.3291,  0.5413],\n",
       "         [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698]],\n",
       "\n",
       "        [[ 0.1970, -0.2512, -0.6867,  2.3483,  0.4105,  0.4855,  1.1432,\n",
       "           0.0955, -0.0180,  0.1363],\n",
       "         [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155,\n",
       "          -0.3412,  0.0888,  0.4907],\n",
       "         [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227,\n",
       "          -0.4734, -0.9524,  0.6621]],\n",
       "\n",
       "        [[-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151,\n",
       "          -0.2171, -0.3291,  0.5413],\n",
       "         [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155,\n",
       "          -0.3412,  0.0888,  0.4907],\n",
       "         [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227,\n",
       "          -0.4734, -0.9524,  0.6621],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227,\n",
       "          -0.4734, -0.9524,  0.6621],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,\n",
       "           1.3961,  0.2879,  0.4053],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,\n",
       "           0.9931,  0.5774, -0.6358],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed your sequences\n",
    "seq_tensor = embed(seq_tensor)\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,  0.9931,\n",
       "          0.5774, -0.6358],\n",
       "        [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,  2.2093,\n",
       "         -0.3803,  1.0325],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-0.9049, -2.2375, -0.5936, -0.4607, -0.6410, -0.8513, -0.8477,  0.1328,\n",
       "         -0.8828,  0.5129],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,  0.9931,\n",
       "          0.5774, -0.6358],\n",
       "        [ 0.2574, -0.9649, -1.3667,  0.3344, -0.1496,  1.3959,  0.5439,  0.6314,\n",
       "          0.3370,  1.6015],\n",
       "        [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,  1.3961,\n",
       "          0.2879,  0.4053],\n",
       "        [ 0.4495, -0.0517,  0.3217, -1.3956, -0.8258, -0.4533, -0.6968,  0.1905,\n",
       "          0.0951, -1.3101],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [ 1.2062,  0.8103, -0.0299, -1.1283,  0.7842,  1.0114, -1.7564,  1.2274,\n",
       "         -0.7476, -1.6700],\n",
       "        [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,  1.3961,\n",
       "          0.2879,  0.4053],\n",
       "        [-1.1101, -0.9401, -1.4768,  0.3649, -0.6038,  0.9148,  1.4016, -0.4053,\n",
       "          1.5718, -0.5589],\n",
       "        [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151, -0.2171,\n",
       "         -0.3291,  0.5413],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,  2.2093,\n",
       "         -0.3803,  1.0325],\n",
       "        [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155, -0.3412,\n",
       "          0.0888,  0.4907],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151, -0.2171,\n",
       "         -0.3291,  0.5413],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.1970, -0.2512, -0.6867,  2.3483,  0.4105,  0.4855,  1.1432,  0.0955,\n",
       "         -0.0180,  0.1363],\n",
       "        [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155, -0.3412,\n",
       "          0.0888,  0.4907],\n",
       "        [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227, -0.4734,\n",
       "         -0.9524,  0.6621],\n",
       "        [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151, -0.2171,\n",
       "         -0.3291,  0.5413],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155, -0.3412,\n",
       "          0.0888,  0.4907],\n",
       "        [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227, -0.4734,\n",
       "         -0.9524,  0.6621],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227, -0.4734,\n",
       "         -0.9524,  0.6621],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,  1.3961,\n",
       "          0.2879,  0.4053],\n",
       "        [-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,  0.9931,\n",
       "          0.5774, -0.6358]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pack them up nicely\n",
    "packed_input = pack_padded_sequence(seq_tensor, seq_lengths.cpu().numpy(), )\n",
    "packed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.5799e-02, -5.1147e-03, -8.9930e-03,  2.9092e-02,  1.5934e-01],\n",
       "        [-4.4151e-02, -1.9077e-01, -6.7441e-02,  4.9654e-02, -2.9302e-01],\n",
       "        [ 9.3984e-02,  2.2068e-02,  3.1124e-02,  5.3917e-02, -5.0069e-02],\n",
       "        [-1.1737e-01, -4.0171e-02, -1.4638e-02, -1.3960e-02,  9.6243e-02],\n",
       "        [-1.0768e-01, -4.0139e-01, -1.2710e-01,  2.3933e-01, -3.3233e-02],\n",
       "        [-8.4471e-02, -4.0406e-02,  1.3541e-02, -1.6869e-02,  2.0592e-02],\n",
       "        [ 7.8929e-02, -2.8576e-02, -1.3652e-02,  2.4513e-02,  2.5850e-01],\n",
       "        [-2.4042e-01, -2.8101e-01, -7.7255e-02,  1.2642e-01, -1.2112e-01],\n",
       "        [-2.1847e-02, -6.5642e-02, -1.3816e-02,  1.9457e-02,  1.3664e-01],\n",
       "        [ 2.2556e-01,  4.0997e-02, -7.9717e-02,  1.9160e-01,  1.2672e-01],\n",
       "        [-3.4199e-01, -1.1423e-01, -1.2054e-01,  3.8834e-02, -2.5071e-02],\n",
       "        [ 1.1592e-01,  4.6574e-02,  8.5085e-02,  1.8202e-01, -1.8496e-01],\n",
       "        [ 2.5322e-02, -1.8444e-04, -1.8394e-02,  9.2575e-02,  2.0497e-01],\n",
       "        [-2.3999e-01, -3.1378e-01, -4.0261e-02,  1.2058e-01, -4.8812e-02],\n",
       "        [-6.5438e-03,  1.4376e-01,  2.7222e-02,  1.9996e-01, -1.1627e-01],\n",
       "        [ 1.3083e-01,  2.7402e-02, -6.4438e-02,  1.5349e-01,  2.6532e-04],\n",
       "        [-1.7168e-01, -2.4962e-01, -9.9952e-02,  1.1480e-01, -3.5238e-01],\n",
       "        [-5.4051e-02, -1.6275e-02,  1.5461e-01,  6.3702e-02,  1.0830e-01],\n",
       "        [-4.0944e-02, -4.0227e-02, -2.4938e-02,  1.9472e-02,  4.4973e-02],\n",
       "        [-1.5357e-02, -5.3793e-01, -6.4661e-02,  2.4575e-01, -2.3989e-01],\n",
       "        [ 3.9988e-02,  2.3675e-02,  1.3729e-01,  9.2015e-02, -4.3830e-03],\n",
       "        [-8.5094e-02, -1.9940e-01, -8.4375e-02,  9.6821e-02, -8.4732e-02],\n",
       "        [-6.0874e-02, -4.5136e-01,  6.9147e-03, -3.3077e-03,  2.2512e-02],\n",
       "        [ 3.3217e-02,  7.6719e-02, -2.5517e-02,  1.5934e-01, -1.4551e-03],\n",
       "        [-1.4335e-02, -1.8599e-01, -3.4028e-02,  1.3975e-01, -1.7636e-02],\n",
       "        [ 6.8401e-02, -1.1217e-01,  4.0819e-02,  3.4343e-02, -8.1345e-02],\n",
       "        [-6.3157e-02, -2.0936e-01,  5.3722e-02,  9.9589e-03,  1.4471e-01],\n",
       "        [ 6.0918e-02,  1.3536e-02, -6.7569e-02,  1.1925e-01, -2.4880e-02],\n",
       "        [ 5.7248e-02, -2.5458e-02,  6.7412e-02,  5.3634e-02,  9.8795e-03],\n",
       "        [ 5.1372e-02,  5.4854e-02, -5.9922e-02,  1.3259e-01,  6.2348e-03],\n",
       "        [-1.4669e-01, -3.3984e-02, -2.4771e-02,  1.3429e-02,  5.1135e-02],\n",
       "        [-2.7960e-02, -6.4551e-02, -1.7693e-02,  3.1238e-02,  1.8107e-01],\n",
       "        [ 9.9326e-02, -2.8121e-02, -2.6798e-02,  5.8794e-02,  2.3223e-01]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "packed_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.5799e-02, -5.1147e-03, -8.9930e-03,  2.9092e-02,  1.5934e-01],\n",
       "         [-4.4151e-02, -1.9077e-01, -6.7441e-02,  4.9654e-02, -2.9302e-01],\n",
       "         [ 9.3984e-02,  2.2068e-02,  3.1124e-02,  5.3917e-02, -5.0069e-02]],\n",
       "\n",
       "        [[-1.1737e-01, -4.0171e-02, -1.4638e-02, -1.3960e-02,  9.6243e-02],\n",
       "         [-1.0768e-01, -4.0139e-01, -1.2710e-01,  2.3933e-01, -3.3233e-02],\n",
       "         [-8.4471e-02, -4.0406e-02,  1.3541e-02, -1.6869e-02,  2.0592e-02]],\n",
       "\n",
       "        [[ 7.8929e-02, -2.8576e-02, -1.3652e-02,  2.4513e-02,  2.5850e-01],\n",
       "         [-2.4042e-01, -2.8101e-01, -7.7255e-02,  1.2642e-01, -1.2112e-01],\n",
       "         [-2.1847e-02, -6.5642e-02, -1.3816e-02,  1.9457e-02,  1.3664e-01]],\n",
       "\n",
       "        [[ 2.2556e-01,  4.0997e-02, -7.9717e-02,  1.9160e-01,  1.2672e-01],\n",
       "         [-3.4199e-01, -1.1423e-01, -1.2054e-01,  3.8834e-02, -2.5071e-02],\n",
       "         [ 1.1592e-01,  4.6574e-02,  8.5085e-02,  1.8202e-01, -1.8496e-01]],\n",
       "\n",
       "        [[ 2.5322e-02, -1.8444e-04, -1.8394e-02,  9.2575e-02,  2.0497e-01],\n",
       "         [-2.3999e-01, -3.1378e-01, -4.0261e-02,  1.2058e-01, -4.8812e-02],\n",
       "         [-6.5438e-03,  1.4376e-01,  2.7222e-02,  1.9996e-01, -1.1627e-01]],\n",
       "\n",
       "        [[ 1.3083e-01,  2.7402e-02, -6.4438e-02,  1.5349e-01,  2.6532e-04],\n",
       "         [-1.7168e-01, -2.4962e-01, -9.9952e-02,  1.1480e-01, -3.5238e-01],\n",
       "         [-5.4051e-02, -1.6275e-02,  1.5461e-01,  6.3702e-02,  1.0830e-01]],\n",
       "\n",
       "        [[-4.0944e-02, -4.0227e-02, -2.4938e-02,  1.9472e-02,  4.4973e-02],\n",
       "         [-1.5357e-02, -5.3793e-01, -6.4661e-02,  2.4575e-01, -2.3989e-01],\n",
       "         [ 3.9988e-02,  2.3675e-02,  1.3729e-01,  9.2015e-02, -4.3830e-03]],\n",
       "\n",
       "        [[-8.5094e-02, -1.9940e-01, -8.4375e-02,  9.6821e-02, -8.4732e-02],\n",
       "         [-6.0874e-02, -4.5136e-01,  6.9147e-03, -3.3077e-03,  2.2512e-02],\n",
       "         [ 3.3217e-02,  7.6719e-02, -2.5517e-02,  1.5934e-01, -1.4551e-03]],\n",
       "\n",
       "        [[-1.4335e-02, -1.8599e-01, -3.4028e-02,  1.3975e-01, -1.7636e-02],\n",
       "         [ 6.8401e-02, -1.1217e-01,  4.0819e-02,  3.4343e-02, -8.1345e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-6.3157e-02, -2.0936e-01,  5.3722e-02,  9.9589e-03,  1.4471e-01],\n",
       "         [ 6.0918e-02,  1.3536e-02, -6.7569e-02,  1.1925e-01, -2.4880e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 5.7248e-02, -2.5458e-02,  6.7412e-02,  5.3634e-02,  9.8795e-03],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 5.1372e-02,  5.4854e-02, -5.9922e-02,  1.3259e-01,  6.2348e-03],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-1.4669e-01, -3.3984e-02, -2.4771e-02,  1.3429e-02,  5.1135e-02],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[-2.7960e-02, -6.4551e-02, -1.7693e-02,  3.1238e-02,  1.8107e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
       "\n",
       "        [[ 9.9326e-02, -2.8121e-02, -2.6798e-02,  5.8794e-02,  2.3223e-01],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0647,  0.3305, -0.2067, -0.1223, -0.2079],\n",
      "        [ 0.0632, -0.0767, -0.2668, -0.2260, -0.0626],\n",
      "        [ 0.0580, -0.1709, -0.2422, -0.2582, -0.0009]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Or if you just want the final hidden state?\n",
    "print (ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0596,  0.2298, -0.0566, -0.1202,  0.0247],\n",
       "         [ 0.0981,  0.0968, -0.2379, -0.3277,  0.0095],\n",
       "         [ 0.0297, -0.0450,  0.0108, -0.1461,  0.1945]],\n",
       "\n",
       "        [[ 0.0719,  0.3170, -0.0687, -0.1164, -0.0988],\n",
       "         [ 0.1125,  0.0234, -0.2327, -0.1314, -0.5589],\n",
       "         [ 0.1497,  0.1363, -0.1713, -0.2054, -0.2830]],\n",
       "\n",
       "        [[ 0.0664,  0.1662, -0.0239, -0.0966, -0.1587],\n",
       "         [ 0.0699, -0.0955, -0.3017, -0.1477, -0.5515],\n",
       "         [ 0.0654, -0.1383,  0.0105, -0.0921,  0.0347]]],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMEMBER: Your outputs are sorted. If you want the original ordering\n",
    "# back (to compare to some gt labels) unsort them\n",
    "_, unperm_idx = perm_idx.sort(0)\n",
    "output = output[unperm_idx]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([[1,1,1,1,1],[2,2,2],[3,3],[4]])\n",
    "l = [len(i) for i in s]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 300])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(25, 300)\n",
    "b = torch.ones(22, 300)\n",
    "c = torch.ones(15, 300)\n",
    "pad_sequence([a, b, c]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 300])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[12,3]].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Let's assume that each element in \"batch\" is a tuple (data, label).\n",
    "    # Sort the batch in the descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "    # Get each sequence and pad it\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    # Also need to store the length of each sequence\n",
    "    # This is later needed in order to unpad the sequences\n",
    "    lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "    # Don't forget to grab the labels of the *sorted* batch\n",
    "    labels = torch.LongTensor(map(lambda x: x[1], sorted_batch))\n",
    "    return sequences_padded, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f1f3f0282930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/tep-EqsBjobE/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;31m# assuming trailing dimensions and type of all the Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# in sequences are same and fetching those from sequences[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0mtrailing_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "pad_sequence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'batch_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-12d235d4f101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/tep-EqsBjobE/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'batch_sizes'"
     ]
    }
   ],
   "source": [
    "pad_packed_sequence(s, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
