{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '_', 'a', 'c', 'd', 'e', 'g', 'i', 'm', 'n', 'r', 's', 't', 'u', 'y']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs = ['gigantic_string','tiny_str','medium_str']\n",
    "\n",
    "# make <pad> idx 0\n",
    "vocab = ['<pad>'] + sorted(set(''.join(seqs)))\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make model\n",
    "embed = nn.Embedding(len(vocab), 10).cpu()\n",
    "lstm = nn.LSTM(10, 5).cpu()\n",
    "lstm.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_seqs = [[vocab.index(tok) for tok in seq] for seq in seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7, 6, 2, 9, 12, 7, 3, 1, 11, 12, 10, 7, 9, 6],\n",
       " [12, 7, 9, 14, 1, 11, 12, 10],\n",
       " [8, 5, 4, 7, 13, 8, 1, 11, 12, 10]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15,  8, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the length of each seq in your batch\n",
    "seq_lengths = torch.LongTensor([len(seq) for seq in vectorized_seqs]).cpu()\n",
    "seq_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  6,  2,  9, 12,  7,  3,  1, 11, 12, 10,  7,  9,  6],\n",
       "        [12,  7,  9, 14,  1, 11, 12, 10,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 8,  5,  4,  7, 13,  8,  1, 11, 12, 10,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long().cpu()\n",
    "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    \n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7,  6,  2,  9, 12,  7,  3,  1, 11, 12, 10,  7,  9,  6],\n",
       "        [ 8,  5,  4,  7, 13,  8,  1, 11, 12, 10,  0,  0,  0,  0,  0],\n",
       "        [12,  7,  9, 14,  1, 11, 12, 10,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SORT YOUR TENSORS BY LENGTH!\n",
    "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
    "seq_tensor = seq_tensor[perm_idx]\n",
    "\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  8, 12],\n",
       "        [ 7,  5,  7],\n",
       "        [ 6,  4,  9],\n",
       "        [ 2,  7, 14],\n",
       "        [ 9, 13,  1],\n",
       "        [12,  8, 11],\n",
       "        [ 7,  1, 12],\n",
       "        [ 3, 11, 10],\n",
       "        [ 1, 12,  0],\n",
       "        [11, 10,  0],\n",
       "        [12,  0,  0],\n",
       "        [10,  0,  0],\n",
       "        [ 7,  0,  0],\n",
       "        [ 9,  0,  0],\n",
       "        [ 6,  0,  0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_tensor = seq_tensor.transpose(0,1) # (B,L,D) -> (L,B,D)\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,\n",
       "           0.9931,  0.5774, -0.6358],\n",
       "         [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,\n",
       "           2.2093, -0.3803,  1.0325],\n",
       "         [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698]],\n",
       "\n",
       "        [[ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [-0.9049, -2.2375, -0.5936, -0.4607, -0.6410, -0.8513, -0.8477,\n",
       "           0.1328, -0.8828,  0.5129],\n",
       "         [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784]],\n",
       "\n",
       "        [[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,\n",
       "           0.9931,  0.5774, -0.6358],\n",
       "         [ 0.2574, -0.9649, -1.3667,  0.3344, -0.1496,  1.3959,  0.5439,\n",
       "           0.6314,  0.3370,  1.6015],\n",
       "         [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,\n",
       "           1.3961,  0.2879,  0.4053]],\n",
       "\n",
       "        [[ 0.4495, -0.0517,  0.3217, -1.3956, -0.8258, -0.4533, -0.6968,\n",
       "           0.1905,  0.0951, -1.3101],\n",
       "         [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [ 1.2062,  0.8103, -0.0299, -1.1283,  0.7842,  1.0114, -1.7564,\n",
       "           1.2274, -0.7476, -1.6700]],\n",
       "\n",
       "        [[-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,\n",
       "           1.3961,  0.2879,  0.4053],\n",
       "         [-1.1101, -0.9401, -1.4768,  0.3649, -0.6038,  0.9148,  1.4016,\n",
       "          -0.4053,  1.5718, -0.5589],\n",
       "         [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151,\n",
       "          -0.2171, -0.3291,  0.5413]],\n",
       "\n",
       "        [[ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698],\n",
       "         [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,\n",
       "           2.2093, -0.3803,  1.0325],\n",
       "         [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155,\n",
       "          -0.3412,  0.0888,  0.4907]],\n",
       "\n",
       "        [[ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151,\n",
       "          -0.2171, -0.3291,  0.5413],\n",
       "         [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698]],\n",
       "\n",
       "        [[ 0.1970, -0.2512, -0.6867,  2.3483,  0.4105,  0.4855,  1.1432,\n",
       "           0.0955, -0.0180,  0.1363],\n",
       "         [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155,\n",
       "          -0.3412,  0.0888,  0.4907],\n",
       "         [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227,\n",
       "          -0.4734, -0.9524,  0.6621]],\n",
       "\n",
       "        [[-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151,\n",
       "          -0.2171, -0.3291,  0.5413],\n",
       "         [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155,\n",
       "          -0.3412,  0.0888,  0.4907],\n",
       "         [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227,\n",
       "          -0.4734, -0.9524,  0.6621],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785,\n",
       "          -0.0957,  0.9062,  0.3698],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227,\n",
       "          -0.4734, -0.9524,  0.6621],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468,\n",
       "          -0.5086, -0.7602, -0.0784],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,\n",
       "           1.3961,  0.2879,  0.4053],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]],\n",
       "\n",
       "        [[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,\n",
       "           0.9931,  0.5774, -0.6358],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844],\n",
       "         [-0.1105, -0.4861,  0.5790,  0.3405,  0.7304,  1.4005,  0.0028,\n",
       "          -0.2637,  0.1818, -1.4844]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embed your sequences\n",
    "seq_tensor = embed(seq_tensor)\n",
    "seq_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,  0.9931,\n",
       "          0.5774, -0.6358],\n",
       "        [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,  2.2093,\n",
       "         -0.3803,  1.0325],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-0.9049, -2.2375, -0.5936, -0.4607, -0.6410, -0.8513, -0.8477,  0.1328,\n",
       "         -0.8828,  0.5129],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,  0.9931,\n",
       "          0.5774, -0.6358],\n",
       "        [ 0.2574, -0.9649, -1.3667,  0.3344, -0.1496,  1.3959,  0.5439,  0.6314,\n",
       "          0.3370,  1.6015],\n",
       "        [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,  1.3961,\n",
       "          0.2879,  0.4053],\n",
       "        [ 0.4495, -0.0517,  0.3217, -1.3956, -0.8258, -0.4533, -0.6968,  0.1905,\n",
       "          0.0951, -1.3101],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [ 1.2062,  0.8103, -0.0299, -1.1283,  0.7842,  1.0114, -1.7564,  1.2274,\n",
       "         -0.7476, -1.6700],\n",
       "        [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,  1.3961,\n",
       "          0.2879,  0.4053],\n",
       "        [-1.1101, -0.9401, -1.4768,  0.3649, -0.6038,  0.9148,  1.4016, -0.4053,\n",
       "          1.5718, -0.5589],\n",
       "        [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151, -0.2171,\n",
       "         -0.3291,  0.5413],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.9024, -0.9464, -0.0745,  1.0031, -0.2675,  0.0685,  0.4278,  2.2093,\n",
       "         -0.3803,  1.0325],\n",
       "        [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155, -0.3412,\n",
       "          0.0888,  0.4907],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151, -0.2171,\n",
       "         -0.3291,  0.5413],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.1970, -0.2512, -0.6867,  2.3483,  0.4105,  0.4855,  1.1432,  0.0955,\n",
       "         -0.0180,  0.1363],\n",
       "        [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155, -0.3412,\n",
       "          0.0888,  0.4907],\n",
       "        [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227, -0.4734,\n",
       "         -0.9524,  0.6621],\n",
       "        [-2.0017, -1.3987,  1.2957,  1.8932,  0.5934, -0.3706,  0.0151, -0.2171,\n",
       "         -0.3291,  0.5413],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [-0.3122, -1.2988,  1.7316, -0.8106,  1.1724,  0.1940, -0.1155, -0.3412,\n",
       "          0.0888,  0.4907],\n",
       "        [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227, -0.4734,\n",
       "         -0.9524,  0.6621],\n",
       "        [ 0.5292,  1.3212, -0.1149, -1.2809,  0.4934,  0.1881, -1.4785, -0.0957,\n",
       "          0.9062,  0.3698],\n",
       "        [ 0.7558,  0.5532,  0.0330, -0.0091, -1.1494, -0.7380, -1.0227, -0.4734,\n",
       "         -0.9524,  0.6621],\n",
       "        [ 0.0148,  0.3304, -0.4568, -1.0534, -1.8018,  2.1466, -0.7468, -0.5086,\n",
       "         -0.7602, -0.0784],\n",
       "        [-1.5850,  0.0554,  0.8987,  0.9379, -2.3707, -0.3422, -0.2109,  1.3961,\n",
       "          0.2879,  0.4053],\n",
       "        [-1.1346,  0.8192,  0.2694, -1.0236, -2.1938,  0.5937, -1.0437,  0.9931,\n",
       "          0.5774, -0.6358]], grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 1, 1, 1, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pack them up nicely\n",
    "packed_input = pack_padded_sequence(seq_tensor, seq_lengths.cpu().numpy())\n",
    "packed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# throw them through your LSTM (remember to give batch_first=True here if you packed with it)\n",
    "packed_output, (ht, ct) = lstm(packed_input)\n",
    "\n",
    "len(packed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0596,  0.2298, -0.0566, -0.1202,  0.0247],\n",
       "         [ 0.0981,  0.0968, -0.2379, -0.3277,  0.0095],\n",
       "         [ 0.0297, -0.0450,  0.0108, -0.1461,  0.1945]],\n",
       "\n",
       "        [[ 0.0664,  0.1662, -0.0239, -0.0966, -0.1587],\n",
       "         [ 0.0699, -0.0955, -0.3017, -0.1477, -0.5515],\n",
       "         [ 0.0654, -0.1383,  0.0105, -0.0921,  0.0347]],\n",
       "\n",
       "        [[ 0.0719,  0.3170, -0.0687, -0.1164, -0.0988],\n",
       "         [ 0.1125,  0.0234, -0.2327, -0.1314, -0.5589],\n",
       "         [ 0.1497,  0.1363, -0.1713, -0.2054, -0.2830]],\n",
       "\n",
       "        [[ 0.0290,  0.0224, -0.0258, -0.2073, -0.0824],\n",
       "         [ 0.0679,  0.0220, -0.1565, -0.0082, -0.3966],\n",
       "         [ 0.0729, -0.2943, -0.2245, -0.3810,  0.0464]],\n",
       "\n",
       "        [[ 0.1318,  0.2027, -0.1826, -0.2431, -0.3489],\n",
       "         [ 0.2006,  0.0599, -0.0218, -0.0396, -0.4671],\n",
       "         [ 0.2130, -0.0439, -0.2591, -0.1935, -0.1182]],\n",
       "\n",
       "        [[ 0.0484,  0.1607, -0.1635, -0.1784,  0.0675],\n",
       "         [ 0.2036,  0.1792, -0.2417, -0.3852, -0.2269],\n",
       "         [ 0.1149, -0.1614, -0.4316,  0.0862, -0.1660]],\n",
       "\n",
       "        [[ 0.0738,  0.0731, -0.1044, -0.0780, -0.0980],\n",
       "         [ 0.2606,  0.1639, -0.2640, -0.0376, -0.4040],\n",
       "         [ 0.0702, -0.1232, -0.3305, -0.1432,  0.1579]],\n",
       "\n",
       "        [[ 0.1652, -0.0031, -0.1138, -0.2623, -0.1195],\n",
       "         [ 0.1229,  0.0686, -0.4305,  0.2126, -0.3315],\n",
       "         [ 0.0580, -0.1709, -0.2422, -0.2582, -0.0009]],\n",
       "\n",
       "        [[ 0.2436,  0.0803, -0.2646, -0.0475, -0.3093],\n",
       "         [ 0.0757,  0.0093, -0.3448, -0.1008,  0.0885],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1200, -0.0226, -0.4329,  0.1757, -0.2723],\n",
       "         [ 0.0632, -0.0767, -0.2668, -0.2260, -0.0626],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0737, -0.0432, -0.3428, -0.1172,  0.1157],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0612, -0.1147, -0.2602, -0.2381, -0.0385],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0671, -0.1995, -0.1761, -0.1049, -0.0842],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1475,  0.1248, -0.2066, -0.1963, -0.3246],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0647,  0.3305, -0.2067, -0.1223, -0.2079],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unpack your output if required\n",
    "output, _ = pad_packed_sequence(packed_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0647,  0.3305, -0.2067, -0.1223, -0.2079],\n",
      "        [ 0.0632, -0.0767, -0.2668, -0.2260, -0.0626],\n",
      "        [ 0.0580, -0.1709, -0.2422, -0.2582, -0.0009]],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Or if you just want the final hidden state?\n",
    "print (ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0596,  0.2298, -0.0566, -0.1202,  0.0247],\n",
       "         [ 0.0981,  0.0968, -0.2379, -0.3277,  0.0095],\n",
       "         [ 0.0297, -0.0450,  0.0108, -0.1461,  0.1945]],\n",
       "\n",
       "        [[ 0.0719,  0.3170, -0.0687, -0.1164, -0.0988],\n",
       "         [ 0.1125,  0.0234, -0.2327, -0.1314, -0.5589],\n",
       "         [ 0.1497,  0.1363, -0.1713, -0.2054, -0.2830]],\n",
       "\n",
       "        [[ 0.0664,  0.1662, -0.0239, -0.0966, -0.1587],\n",
       "         [ 0.0699, -0.0955, -0.3017, -0.1477, -0.5515],\n",
       "         [ 0.0654, -0.1383,  0.0105, -0.0921,  0.0347]]],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# REMEMBER: Your outputs are sorted. If you want the original ordering\n",
    "# back (to compare to some gt labels) unsort them\n",
    "_, unperm_idx = perm_idx.sort(0)\n",
    "output = output[unperm_idx]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 2, 1]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.array([[1,1,1,1,1],[2,2,2],[3,3],[4]])\n",
    "l = [len(i) for i in s]\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 300])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(25, 300)\n",
    "b = torch.ones(22, 300)\n",
    "c = torch.ones(15, 300)\n",
    "pad_sequence([a, b, c]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 300])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 300])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[12,3]].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Let's assume that each element in \"batch\" is a tuple (data, label).\n",
    "    # Sort the batch in the descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "    # Get each sequence and pad it\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True)\n",
    "    # Also need to store the length of each sequence\n",
    "    # This is later needed in order to unpad the sequences\n",
    "    lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "    # Don't forget to grab the labels of the *sorted* batch\n",
    "    labels = torch.LongTensor(map(lambda x: x[1], sorted_batch))\n",
    "    return sequences_padded, lengths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f1f3f0282930>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/tep-EqsBjobE/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;31m# assuming trailing dimensions and type of all the Tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# in sequences are same and fetching those from sequences[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0mmax_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0mtrailing_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "pad_sequence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'batch_sizes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-12d235d4f101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/tep-EqsBjobE/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_length\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'batch_sizes'"
     ]
    }
   ],
   "source": [
    "pad_packed_sequence(s, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
